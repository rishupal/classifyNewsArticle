{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is file number 1.\n"
     ]
    }
   ],
   "source": [
    "#Sriramajayam\n",
    "import re\n",
    "training_file = open(\"test_document.txt\",encoding=\"utf-8\")\n",
    "training_data = training_file.read()\n",
    "training_data_doc_contents_list = re.findall(r\"ID tr_doc_[0-9]+[\\n]TEXT(.*?)[\\n]EOD\", training_data)\n",
    "print(training_data_doc_contents_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_file = open(\"testing_docs.txt\",encoding=\"utf-8\")\n",
    "# testing_data = testing_file.read()\n",
    "# testing_data_doc_contents_list = re.findall(r\"ID te_doc_[0-9]+[\\n]TEXT(.*?)[\\n]EOD\", testing_data)\n",
    "# print(testing_data_doc_contents_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # List of list for training data features' list\n",
    "# training_data_feature_list=[]\n",
    "# for each in training_data_doc_contents_list:\n",
    "#     vectorizer = TfidfVectorizer(stopwords='english')\n",
    "#     content_list = []\n",
    "#     if(len(each)>2):\n",
    "#         X = vectorizer.fit_transform([each])\n",
    "#         content_list = vectorizer.get_feature_names()\n",
    "#     training_data_feature_list.append(content_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # List of list for testing data features' list\n",
    "# testing_data_feature_list =[]\n",
    "# for each in testing_data_doc_contents_list:\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#     content_list = []\n",
    "#     if(len(each)>2):\n",
    "#         X = vectorizer.fit_transform(C1_list)\n",
    "#         content_list = vectorizer.get_feature_names()\n",
    "#     testing_data_feature_list.append(content_list)\n",
    "\n",
    "# print(testing_data_feature_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr_doc_00001</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr_doc_00002</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr_doc_00003</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr_doc_00004</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc_id topic_id\n",
       "0  tr_doc_00001       C1\n",
       "1  tr_doc_00002       C1\n",
       "2  tr_doc_00003       C1\n",
       "3  tr_doc_00004       C2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_data_df = pd.read_csv(\"test_labels.txt\",sep=' ',names=['doc_id','topic_id'])\n",
    "training_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df['content'] = pd.Series(training_data_doc_contents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr_doc_00001</td>\n",
       "      <td>C1</td>\n",
       "      <td>This is file number 1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr_doc_00002</td>\n",
       "      <td>C1</td>\n",
       "      <td>This is file number 2 file.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr_doc_00003</td>\n",
       "      <td>C1</td>\n",
       "      <td>This is file number 3.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr_doc_00004</td>\n",
       "      <td>C2</td>\n",
       "      <td>This is file number 4.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc_id topic_id                       content\n",
       "0  tr_doc_00001       C1        This is file number 1.\n",
       "1  tr_doc_00002       C1   This is file number 2 file.\n",
       "2  tr_doc_00003       C1        This is file number 3.\n",
       "3  tr_doc_00004       C2        This is file number 4."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def stemmer(doc):\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmed_doc = stemmer.stem(doc)\n",
    "    return stemmed_doc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(CountVectorizer):\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_name ==  C1\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import itertools\n",
    "topic_groups = training_data_df.groupby('topic_id')\n",
    "vectorizer = StemmedTfidfVectorizer(stop_words='english', ngram_range=(1,3),analyzer='word')\n",
    "#save_group_content = open(\"29406242_group.txt\", 'a')\n",
    "file_index = 1\n",
    "\n",
    "for group_name, each_topic in topic_groups:  \n",
    "    \n",
    "    if group_name == 'C1':\n",
    "        x = vectorizer.fit_transform(each_topic['content'])\n",
    "        mapping_dict = vectorizer.vocabulary_\n",
    "\n",
    "        coordinates = x.tocoo()\n",
    "        save_term_doc_matrix_name = \"29406242_tfidf_\"+ str(file_index) + \".txt\"\n",
    "        save_term_doc_matrix = open(save_term_doc_matrix_name, 'w')\n",
    "\n",
    "        for i, j, v in itertools.zip_longest(coordinates.row, coordinates.col, coordinates.data):\n",
    "            save_term_doc_matrix.write(str(i) + ':' + str(j) + ':' + str(v)+ \"\\n\")\n",
    "        save_term_doc_matrix.write(\"***************************************************************\")\n",
    "        save_term_doc_matrix.close()\n",
    "\n",
    "        save_term_index_map_name = \"29406242_vocab_\"+ str(file_index) + \".txt\"\n",
    "        save_term_index_map = open(save_term_index_map_name, 'w',encoding='utf-8')\n",
    "        file_index += 1\n",
    "        for term, index in sorted(mapping_dict.items()):\n",
    "            save_term_index_map.write(term + \": \" + str(index) + \"\\n\")\n",
    "        save_term_index_map.write(\"***************************************************************\")\n",
    "        save_term_index_map.close()       \n",
    "\n",
    "        print(\"group_name == \",group_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
